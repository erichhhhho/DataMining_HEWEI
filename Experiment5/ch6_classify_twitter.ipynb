{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Modified by HE WEI Oct 27,2017\"\"\"\n",
    "\n",
    "# Labelling the class values for the twitter dataset.\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "# In Pycharm\n",
    "input_filename = os.path.join(os.path.abspath('Data/'), \"twitter\\python_tweets.json\")\n",
    "labels_filename = os.path.join(os.path.abspath('Data/'), \"twitter\\python_classes.json\")\n",
    "replicable_dataset = os.path.join(os.path.abspath('Data/twitter'), \"replicable_dataset.json\")\n",
    "\n",
    "#in Jupyter\n",
    "# input_filename = os.path.join(os.path.abspath('../Data/'), \"twitter\\python_tweets.json\")\n",
    "# classes_filename = os.path.join(os.path.abspath('../Data/'), \"twitter\\python_classes.json\")\n",
    "# replicable_dataset = os.path.join(os.path.abspath('../Data/twitter'), \"replicable_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 tweets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweets = []\n",
    "with open(input_filename) as inf:\n",
    "    for line in inf:\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        tweets.append(json.loads(line)['text'])\n",
    "print(\"Loaded {} tweets\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(labels_filename) as inf:\n",
    "    labels = json.load(inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_samples = min(len(tweets), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_tweets = [t.lower() for t in tweets[:n_samples]]\n",
    "# labels = labels[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# import numpy as np\n",
    "# y_true = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"{:.1f}% have class 1\".format(np.mean(y_true == 1) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# with open(replicable_dataset) as inf:\n",
    "#     tweet_ids = json.load(inf)\n",
    "# \n",
    "# actual_labels = []\n",
    "# label_mapping = dict(tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import twitter\n",
    "# consumer_key = \"e7u57o1hv5AzKX1lqS9UxQjL0\"\n",
    "# consumer_secret = \"2vgeDa5H5K5JksA9X1x5ZTXCk2HPTbThONyM11F46rt5ButOyB\"\n",
    "# access_token = \"469088235-dFCb8IVWnJNxqEzZ2pL9bw0sE44hZWn9Y2GJVo8I\"\n",
    "# access_token_secret = \"wYe10maJG5yNNaY7FmajPisPeg9HsXsTiFWd8d8S8kwOw\"\n",
    "# authorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)\n",
    "# t = twitter.Twitter(auth=authorization)\n",
    "# \n",
    "# all_ids = [tweet_id for tweet_id, label in tweet_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "class NLTKBOW(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [{word: True for word in word_tokenize(document)}\n",
    "                 for document in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\q\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\DataMining\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score \n",
    "#from sklearn.model_selection import cross_val_score \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('bag-of-words', NLTKBOW()),\n",
    "                     ('vectorizer', DictVectorizer()),\n",
    "                     ('naive-bayes', BernoulliNB())\n",
    "                     ])\n",
    "#print(tweets)\n",
    "print(len(labels))\n",
    "scores = cross_val_score(pipeline, tweets, labels, scoring='f1')\n",
    "print(\"Score: {:.3f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26666667,  0.63157895,  0.5       ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "name": "",
  "signature": "sha256:0572f58810bcde678e56e4c78ad5181092637198a0b58fe081990b0b1540ee52"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
